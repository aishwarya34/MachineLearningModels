{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "#import h5py\n",
    "#import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "#from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "#from cnn_utils import *\n",
    "## Load Libraries\n",
    "#import os\n",
    "#import requests, zipfile, io\n",
    "#import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "#%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the current directory where files will get downloaded\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into platform\n",
    "#url = requests.get('https://he-s3.s3.amazonaws.com/media/hackathon/deep-learning-challenge-1/identify-the-objects/a0409a00-8-dataset_dp.zip')\n",
    "#data = zipfile.ZipFile(io.BytesIO(url.content))\n",
    "#data.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the files have been download in current directory\n",
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load files\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()   #Used by hackerearth.com to evaluat submissions based on weighted F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before splitting , count the number of unique classes in training dataset\n",
    "classes = train['label'].nunique()\n",
    "print(classes)\n",
    "classnames = pd.unique(train['label'])\n",
    "print(classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Im going to just randomly shuffle dataset and split it into train-test 4:1 ratio\n",
    "df = pd.DataFrame(train)\n",
    "#print(df)\n",
    "train_random = df.sample(frac=0.8,random_state=200)\n",
    "test_random = df.drop(train_random.index)\n",
    "print(test_random.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_random['image_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to read train and test image\n",
    "TRAIN_PATH = 'train_img/'\n",
    "TEST_PATH = 'test_img/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read images as arrays\n",
    "def read_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (64,64)) # you can resize to  (128,128) or (256,256)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig = []\n",
    "X_test_orig = []\n",
    "\n",
    "# looping over images in the directory\n",
    "#for img in tqdm(train_random['image_id'].values):\n",
    "\n",
    "for img in train_random['image_id'].values:\n",
    "    X_train_orig.append(read_image(TRAIN_PATH+'{}.png'.format(img)))\n",
    "for img in test_random['image_id'].values:\n",
    "    X_test_orig.append(read_image(TRAIN_PATH+'{}.png'.format(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data X\n",
    "X_train_orig = np.array(X_train_orig)\n",
    "X_test_orig = np.array(X_test_orig)\n",
    "print(X_train_orig.shape)\n",
    "print(X_test_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_random['label'].values\n",
    "test_labels = test_random['label'].values\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class names from String to index values\n",
    "for tstr in train_labels:\n",
    "    train_labels[list(train_labels).index(tstr)] = list(classnames).index(tstr)\n",
    "print(train_labels)\n",
    "for tstr in test_labels:\n",
    "    test_labels[list(test_labels).index(tstr)] = list(classnames).index(tstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(data, countclass):\n",
    "    Y = np.zeros([data.shape[0],countclass])\n",
    "    print(Y.shape)\n",
    "    #for notoeye in np.nditer(data ,[\"refs_ok\"]):\n",
    "    for notoeye in data:\n",
    "        newrow = np.eye(data.shape[0])[notoeye-1]\n",
    "        newrow = np.expand_dims(newrow, axis=1)\n",
    "        np.concatenate((Y, newrow) , axis=1) \n",
    "    print(Y.shape)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.reshape(-1)\n",
    "print(train_labels.shape[0])\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data Y by performing one hot encoding\n",
    "print(train_labels.shape[0])\n",
    "Y_train = convert_to_one_hot(train_labels, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = convert_to_one_hot(test_labels, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "conv_layers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_mini_batches\n",
    "# minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "def random_mini_batches(X, Y, minibatch_size = 64, seed = 0):\n",
    "    m = X_train.shape[0]\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Shuffle (X, Y)\n",
    "    permutation = np.random.permutation(m)\n",
    "    X_shuffled = X[permutation,:,:,:]\n",
    "    Y_shuffled = Y[permutation,:]\n",
    "\n",
    "    #Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/minibatch_size)\n",
    "    for k in range(0,num_complete_minibatches):\n",
    "        mini_batch_x = X_shuffled[k*minibatch_size : k*minibatch_size+minibatch_size ,:,:,:]\n",
    "        mini_batch_y = Y_shuffled[k*minibatch_size : k*minibatch_size+minibatch_size ,:]\n",
    "        mini_batch = (mini_batch_x, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % minibatch_size != 0:\n",
    "        mini_batch_x = X_shuffled[num_complete_minibatches*minibatch_size : m ,:,:,:]\n",
    "        mini_batch_y = Y_shuffled[num_complete_minibatches*minibatch_size : m ,:]\n",
    "        mini_batch = (mini_batch_x, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_H0 -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (â‰ˆ2 lines)\n",
    "    X = tf.placeholder(tf.float32 , shape = (None, n_H0, n_W0, n_C0) )\n",
    "    Y = tf.placeholder(tf.float32 , shape = (None, n_y) )\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]\n",
    "                        W2 : [2, 2, 8, 16]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                              # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 2 lines of code)\n",
    "    W1 = tf.get_variable(\"W1\", [4,4,3,1], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    \"\"\"\n",
    "    W1 = tf.get_variable(\"W1\", [4,4,3,96], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable(\"W2\", [5,5,96,256], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W3 = tf.get_variable(\"W3\", [3,3,256,384], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W4 = tf.get_variable(\"W4\", [3,3,384,384], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W5 = tf.get_variable(\"W5\", [3,3,384,256], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\"\"\"\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "   # parameters = {\"W1\": W1,\n",
    "   #               \"W2\": W2,\n",
    "   #               \"W3\": W3,\n",
    "   #               \"W4\": W4,\n",
    "   #               \"W5\": W5}\n",
    "    \n",
    "    parameters = {\"W1\": W1}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    \"\"\"W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    W4 = parameters['W4']\n",
    "    W5 = parameters['W5']\"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # CONV2D: stride of 1, padding 'VALID'\n",
    "    Z1 = tf.nn.conv2d( X, W1, strides = [1,1,1,1], padding = 'VALID')\n",
    "    # ?? have not taken b1 parameter\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    \"\"\"\n",
    "    # MAXPOOL: window 3x3, sride 2, padding 'VALID'\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # MAXPOOL: window 2x2, stride 1, padding 'VALID'\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,2,2,1], strides = [1,1,1,1], padding = 'VALID')\n",
    "    # CONV2D: filters W3, stride 1, padding 'SAME'\n",
    "    Z3 = tf.nn.conv2d(P2,W3, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    # CONV2D: filters W4, stride 1, padding 'SAME'\n",
    "    Z4 = tf.nn.conv2d(A3,W4, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # RELU\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    # CONV2D: filters W5, stride 1, padding 'SAME'\n",
    "    Z5 = tf.nn.conv2d(A4,W5, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # RELU\n",
    "    A5 = tf.nn.relu(Z5)\n",
    "    # MAXPOOL: window 3x3, stride 2, padding 'VALID'\n",
    "    P3 = tf.nn.max_pool(A5, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    # FLATTEN\n",
    "    P3 = tf.contrib.layers.flatten(P3)\n",
    "    # FULLY-CONNECTED with non-linear activation function.\n",
    "    A6 = tf.contrib.layers.fully_connected(P3, 4096)\n",
    "    A7 = tf.contrib.layers.fully_connected(A6, 4096)    \n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    Z8 = tf.contrib.layers.fully_connected(A7, 6, activation_fn=None)\n",
    "    \"\"\"\n",
    "    # FLATTEN\n",
    "    P1 = tf.contrib.layers.flatten(A1)\n",
    "    Z8 = tf.contrib.layers.fully_connected(P1, 25, activation_fn=None)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z8, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z8 -- output of forward propagation (output of the last LINEAR unit), of shape (25, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z8\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits = Z8, labels = Y) )\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n",
    "          num_epochs = 100, minibatch_size = 64, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z8 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z8, Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict = {X:minibatch_X , Y:minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        #plt.plot(np.squeeze(costs))\n",
    "        #plt.ylabel('cost')\n",
    "        #plt.xlabel('iterations (per tens)')\n",
    "        #plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        #plt.show()\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z8, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test, num_epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
